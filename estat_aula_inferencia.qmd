---
title: "Aula Inferência Estatística"
format: html
---

```{r}
#| echo: false

library(latex2exp)
library(scales)

colorArea <- function(from, to, density, ..., col="blue", dens=NULL){
  y_seq <- seq(from, to, length.out=500)
  d <- c(0, density(y_seq, ...), 0)
  polygon(c(from, y_seq, to), d, col=col, density=dens)
}
colorAreaShade <- function(from, to, density, ..., col="blue", dens=NULL){
  y_seq <- seq(from, to, length.out=500)
  d <- c(0, density(y_seq, ...), 0)
  polygon(c(from, y_seq, to), d, col=col)
}
```
## População e Amostra
Até agora estávamos trabalhando um espaço de probabilidades $\left(\Omega, \mathcal F, f(x;\theta)\right)$ completamente definido que nos permite computar probabilidades sobre eventos definidos no universo de possíveis resultados $\Omega$.

Agora vamos nos concentrar no problema inverso. Vamos observar realizações de uma certa variável aleatória, uma amostra finita 
\begin{align*}
    \mathbf x = \{x_1,x_2,\dots,x_n\}.
\end{align*}
Essas realizações foram sorteadas de uma população $X$, cuja distribuição de probabiliades $f(x;\theta)$ é desconhecida.

Com base nas informações da amostra, tentaremos recuperar informações (por exemplo valor de parâmetros) sobre o modelo (o espaço de probabilidades) que gerou esses dados.

Por inferência estatística entendemos uma coleção de práticas (e o sentido é mais amplo do que apenas métodos matemáticos) cujo objetivo é obter estimativas (palpites) e quantificar nossa incerteza acerca dessas estimativas sobre características de uma população idealizada.

::: {.callout-tip title="Amostra aleatória"}
As variáveis aleatórias $X_1,\dots,X_n$ são chamadas de amostra aleatória de tamanho $n$ de uma população $f(x)$ se
  
  - $X_1,\dots,X_n$ são variáveis aleatórias independentes e
  - a função densidade (ou massa) de probabilidade de cada $X_i$ é a mesma $f(x)$.
  
Alternativamente, podemos dizer que $X_1,\dots,X_n$ são variáveis aleatórias independentes e identicamente distribuídas com fdp ou fmp $f(x)$. Daí abreviamos como variáveis aleatórias iid. 
:::

Uma estatística é qualquer função baseada nos valores da amostra (e da quantidade de observações nessa amostra). 

Suponha que observamos uma amostra aleatória $x= \{x_1,\dots, x_n\}$. São exemplos de estatísticas

  - a média aritmética $\bar x$,
  - a variância amostral $s^2$,
  - a moda amostral,
  - a mediana (e todas as outras estatísticas de ordem),
  - momentos em geral (ex.: assimetria, curtose)
  - a distância interquartílica $q_3-q_1$ (usada na construção do box-plot),
  - o valor $1$.

## Distribuição da Média Amostral

Tome a média amostral $\bar x$.
\begin{align*}
    \bar x = \frac{x_1 + x_2 + \dots + x_n}{n}
\end{align*}
Ela é uma função das realizações da variável aleatória $X$, portanto também é uma variável aleatória por si própria (com características potencialmente diferentes da população original $X$). 

  - Se $\bar x$ é uma v.a. então ela deve ter uma distribuição de probabilidades que atribui pesos relativos a suas realizações.
  - Para uma dada amostra de tamanho $n$, $x^{(1)}=\{x_j^{(1)}\}_{j=1}^n$, o valor de $\bar x^{(1)}$ é sempre o mesmo. Mas se os valores $x_j^{(1)}$ sorteados fosse diferentes, o valor calculado em $\bar x^{(1)}$ também seria alterado.
  - Portanto a distribuição da estatística depende de possíveis valores $\bar x$ (não observados!) que poderiam ser computados com diferentes amostras $x^{(1)}, x^{(2)}, \dots$.
  - Por isso nos referimos à distribuição de amostragem da estatística.

### Valor esperado $E(\bar X)$

Seja $x=\{X_1, X_2, \dots, X_n\}$ uma amostra aleatória retirada de uma população $X$ com média $\mu$ e variância $\sigma^2$. A média amostral é 
\begin{align*}
    \bar X = \frac{X_1 + X_2 + \dots + X_n}{n}.
\end{align*}
Estamos interessados no valor esperado da média amostral
\begin{align*}
  E(\bar X) = E\left(\frac{X_1 + X_2 + \dots + X_n}{n}\right)
\end{align*}
Lembrando que $E(Y+W)=E(Y)+E(W)$ e que $E(aY)=aE(Y)$, podemos escrever
\begin{align*}
  E(\bar x) = \frac{1}{n}\left[E(X_1)+E(X_2)\dots E(X_n)\right].
\end{align*}
Agora usamos o fato de $x$ ser uma amostra aleatória e todos os $X_j$ terem sido sorteados da mesma distribuição de probabilidades. Ou seja, $E(X_1)=E(X_2)=\dots=E(X_n)=\mu$.
\begin{align*}
  E(\bar X) &= \frac{1}{n}\left[E(X_1)+E(X_2)\dots E(X_n)\right] \\
  &=\frac{1}{n}(\mu+\mu+\dots+\mu) = \frac{n\mu}{n} = \mu
\end{align*}
Portanto o valor esperado da média amostral é a verdadeira média populacional.

```{r}
#| echo: true

beta = 3
n = 5
Msim = 10
xBars = rep(0,Msim)
for (j in 1:Msim){
  xBars[j] = mean(rexp(n, rate=1/beta))
}
main = paste(Msim,"estimativas da média \n baseadas em amostras de tamanho",n)
plot(1:Msim, xBars, main=main)
abline(h=beta)
abline(h=mean(xBars), lty=2)
```

```{r}
#| echo: false
beta = 3
n = 5
Msim = 100
xBars = rep(0,Msim)
for (j in 1:Msim){
  xBars[j] = mean(rexp(n, rate=1/beta))
}
main = paste(Msim,"estimativas da média \n baseadas em amostras de tamanho",n)
plot(1:Msim, xBars, main=main)
abline(h=beta)
abline(h=mean(xBars), lty=2)
```

### Variância $Var(\bar X)$

Agora estamos interessados na variância da média amostral $Var(\bar X)$. Aqui iremos observar uma diferença crucial quando comparada à variância populacional $Var(X)=\sigma^2$.

\begin{align*}
  Var(\bar X) &= Var\left(\frac{X_1+X_2+\cdots X_n}{n}\right) \\
  &=\frac{1}{n^2}Var\left(X_1+X_2+\cdots X_n\right) 
\end{align*}
Para duas variáveis aleatórias quaisquer, $Var(Y+W)=Var(Y)+Var(W)+2Cov(Y,W)$. Como $\{X_1,X_2,\dots,X_n\}$ é uma amostra aleatória, $X_i$ é independente de $X_j$ para $i\neq j$, portanto $Cov(X_i,X_j)=0$.
\begin{align*}
  Var(\bar X) = \frac{1}{n^2}\left[Var(X_1)+Var(X_2)+\cdots+Var(X_n)\right]
\end{align*}
Como a amostra é identicamente distribuida, $Var(X_1)=Var(X_2)=\cdots=Var(X_n)=\sigma^2$. E temos
\begin{align*}
  Var(\bar X) = \frac{1}{n^2}\left(\sigma^2+\sigma^2+\cdots+\sigma^2\right) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}
\end{align*}

Portanto a variância da média amostral é a verdadeira variância populacional, dividida pelo tamanho da amostra. Quanto maior o tamanho amostral, menos variável vai ser a estimativa da média.

### Simulação
Tome uma variável aleatória $X$ com distribuição Poisson e parâmetro $\lambda=2$. Sua função massa de probabilidade tem a forma
\begin{align*}
  Pr(X=k) = \frac{e^{-2}2^k}{k!}, \quad k=0,1,\dots.
\end{align*}

```{r}
#| echo: false
#| dev: png

lambda = 2
x = 0:10
par(mgp=c(2,1,0),mar=c(3, 3, 2, 1)+0.1)
plot(x, dpois(x,lambda), t='h', 
     ylim=c(0,0.4), 
     lwd=2,
     main=TeX(r"( X\sim\textbf{Pois.}(2) )"),
     ylab=TeX(r"( \Pr(X=x) )"),
     xlab=TeX(r"( x )"),
     xaxt='n')
axis(1, at=x)
points(x,dpois(x,lambda),pch=16,cex=1,col="black")
```

Vamos simular uma amostra aleatória de tamanho $n=500$, retirada de uma distribuição Poisson com parâmetro $\lambda=2$ e, construir o histograma normalizado. Essa é uma aproximação da distribuição populacional da variável aleatória $X\sim Pois.(\lambda)$.

```{r}
#| echo: true
#| dev: png

lambda = 2 #<1>
n = 500 #<2>
Sim = rpois(n, lambda) #<3>
main = paste("Histograma de ",n," realizações de X~Pois.(",lambda,")",sep="") #<4>
hist(Sim, prob=T, main=main) #<4>
```
1. Escolhemos o valor do o verdadeiro parâmetro $\lambda$.
2. Determinamos o tamanho amostral $n$.
3. Sorteamos $n$ realizações de $X\sim Pois(\lambda)$ usando a função `rpois()`.
4. Construimos o histograma normalizado.

Agora vamos simular 500 diferentes amostras com tamanho $n=5$, computar a média amostral para cada amostra, e construir o histograma dessas estimativas da média.

```{r}
#| tidy: false
lambda = 2 #<1>
n = 5 #<2>
NSim = 500 #<3>
xMedias = vector(length=NSim) #<4>
for (j in 1:NSim){ #<5>
  Sim = rpois(n, lambda) #<5>
  xMedias[j] = mean(Sim) #<5>
} #<5>
main = paste("Histograma de ", NSim, #<6>
           " médias calculadas de amostras n=", n, #<6>
           " \n retiradas de uma população X~Pois.(",lambda,")",sep="") #<6>
hist(xMedias, prob=T,main=main) #<6>
```
1. Verdadeiro valor do parâmetro $\lambda$.
2. Número de observações $n$ em cada amostra individual.
3. Quantidade $NSim$ de amostras com tamanho $n$ que serão simuladas. 
4. Teremos $NSim$ estimativas $\bar x$ para a média populacional. Inicializamos um vetor com $NSim$ entradas para guardar essas estimativas.
5. Realizamos a simulação.
6. Construimos o histograma normalizado.

Repare que a forma do histograma das estimativas $\bar x$ é diferente da forma do histograma das realizações individuais $x$. Notadamente o gráfico é menos assimétrico.

Agora vamos repetir esse exercício para diferentes tamanhos amostrais $n$. Quanto maior $n$, mais a distribuição de $\bar x$ irá se aproximar de algo simétrico ao redor da verdadeira média $E(X)$.

```{r}
nvec = c(3,5,10,20,50,100,200,500,1000) #<1>
par(mfrow=c(3,3)) #<2>
for (j in 1:(3*3)){ #<3>
  for (i in 1:NSim){ #<4>
    Sim = rpois(nvec[j], lambda) #<4>
    xMedias[i] = mean(Sim) #<4>
  } #<4>
  hist(xMedias, prob=T, xlab='',ylab='', main=paste("n=",nvec[j])) #<5>
} #<3>
```
1. Especificamos um vetor com diferentes valores para $n$
2. Os gráficos serão dispostos em um grid 3x3.
3. Para cada tamanho amostral `nvec[j]`
4. Realizamos a simulação.
5. Desenhamos o histograma.

```{r}
#| echo: false
xMedias = matrix(nrow=NSim,ncol=length(nvec))
for (j in 1:(3*3)){ #<3>
  for (i in 1:NSim){
    Sim = rpois(nvec[j], lambda)
    xMedias[i,j] = mean(Sim)
  }
}
boxplot(xMedias, xlab='',ylab='')
```

::: {.callout-warning title="Exercício" collapse=true}
Repita o exercício de simulação de estimativas da média acima para diferentes distribuições de probabilidade.
:::

## Teorema do Limite Central

> I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the the law of frequency of error. The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along. (Sir Francis Galton, 1889, Natural Inheritance.)

Da discussão anterior nós percebemos que 

::: {.callout-tip title="Teorema do Limite Central (TLC)"}
Para amostras aleatórias simples [iid] $(X_1,\dots,X_n)$ retiradas de uma população com média $\mu$ e variância $\sigma^2$ finita, a distribuição amostral da média $\bar X$ aproxima-se, para $n$ grande, de uma distribuição normal, com média $\mu$ e variância $\sigma^2/n$.
\begin{align*}
  \lim_{n\rightarrow\infty} \bar X_n \sim N(\mu, \sigma^2/n).
\end{align*}
:::

  - Foi o que observamos na simulação anterior.
  - O surpreendente é que começamos com suposições de independência e variância finita e acabamos com normalidade (estritamente, a convergência em distribuição para uma Normal).
  - Não importando a distribuição da população (se ela for "bem comportada"), a distribuição das estimativas da média (em repetidas amostras) se aproxima de uma Normal conforme o tamanho amostral $n$ cresce.
  - Esse resultado nos permite fazer inferência (intervalos de confiança, testes de hipótese) sobre a média de uma população cuja distribuição é desconhecida com base nos quantis de uma distribuição Normal.

Quanto maior o tamanho amostral $n$, menor é a variância da distribuição de $\bar X$.
  $$ \bar X \sim N\left(\mu, \frac{\sigma^2}{n}\right)$$
Fica bem claro do box-plot [@colocar_referencia] que a distribuição "colapsa" ao redor da média $\mu$.

Mas é possível normalizar uma variável aleatória de modo que a transformação tenha média $0$ e variância $1$.

::: {.callout-tip title="Corolário"}
Se $(X_1,\dots, X_n)$ for uma amostra aleatória simples da população $X$ com média $\mu$ e variância $\sigma^2$ finita, e $\bar X = (X_1+\cdots+X_n)/n$, então para valores grandes de $n$
    \begin{align*}
        Z = \frac{\bar X-\mu}{\sigma/\sqrt{n}}\sim N(0,1).
    \end{align*}
:::

::: {.callout-note title="Exemplo" collapse=true}
Quando a população em si tem distribuição Normal, o TLC vale exatamente (não é uma aproximação) para qualquer tamanho amostral $n$.^[Isso vem do fato da soma de v.a.s Normais ter também distribuição Normal. Ver função geradora de momentos.]

Retiramos uma amostra aleatória de tamanho $n=20$ de uma variável aleatória Normal $X\sim N(2,3^2)$. Qual é a probabilidade de que $\bar x>1.5$? 

Nesse caso $\bar X \sim N(\mu,\sigma^2/n)$, ou seja
$$
  \bar X \sim N\left(2,\frac{3^2}{20}\right)
$$
Queremos $\Pr(\bar X>1.5)$. Portanto
$$
  \gamma = \Pr(\bar X>1.5) = 1-\Pr(\bar X\leq1.5)
$$
No R podemos calcular diretamente.
```{r}
1-pnorm(1.5,mean=2,sd=3/sqrt(20))
```
```{r}
#| echo: false


par(mgp=c(2,1,0),mar=c(3, 3, 2, 1)+0.1)
sigma = sd=3/sqrt(20)
mu = 2
x = seq(from=-3*sigma+mu, to=mu+3*sigma, length=1000)
plot(x, dnorm(x, mean=mu, sd=sigma), t='l', lwd=2, col=1, main="", ylab="")
colorArea(from=1.5,to=10, dnorm, mean=mu, sd=sigma, col=alpha('black',0.25))
legend('topleft', legend=c("Pr(Xbar>1.5)"),  
       fill = c("gray"))
```

Para trabalhar com a Normal Padrão devemos operar a desigualdade para padronizar $X$.
\begin{align*}
  \gamma &= 1-\Pr(\bar X \leq 1.5) \\
  &= 1-\Pr\left(\frac{\bar X-\mu}{\sigma/\sqrt{n}}\leq \frac{1.5-2}{3/\sqrt{20}}\right) \\
  & = 1-\Pr(Z\leq-0.745)
\end{align*}
```{r}
1-pnorm(-0.745)
```

Ainda não conseguiriamos consultar na tabela da distribuição Normal Padrão (que só fornece $\Pr(Z\leq z)$ para $z\geq0$. Da simetria da Normal, podemos escrever  
\begin{align*}
  \gamma &= 1-\Pr(Z\leq-0.745)=1-\Pr(Z>0.745) \\
  &=1-(1-\Pr(Z\leq 0.745)) = \Pr(Z\leq 0.745)
\end{align*}

```{r}
pnorm(0.745)
```
```{r}
par(mgp=c(2,1,0),mar=c(3, 3, 2, 1)+0.1)
sigma = 1
mu = 0
x = seq(from=-3*sigma+mu, to=mu+3*sigma, length=1000)
plot(x, dnorm(x, mean=mu, sd=sigma), t='l', lwd=2, col=1, main="", ylab="")
colorArea(from=-4,to=0.745, dnorm, mean=mu, sd=sigma, col=alpha('black',0.25))
legend('topleft', legend=c("Pr(Z<0.745)"),  
       fill = c("gray"))
```

Podemos simular a situação. 

```{r}
mu = 2
sigma = 3
n = 20
Msim = 1000 #<1>
xMedias = rep(0,Msim)
for (jsim in 1:Msim){
  sim = rnorm(n, mean=mu, sd=sigma)
  xMedias[jsim] = mean(sim)
}
xcrit = 1.5
mean(xMedias>xcrit)
```
```{r}
#| echo: false


x = seq(from=-3*sigma/sqrt(n)+mu, to=mu+3*sigma/sqrt(n), length=Msim)
hist(xMedias, breaks = sqrt(Msim),prob=T)
abline(v=xcrit,lw=2,lty=2)
lines(x, dnorm(x, mean=mu, sd=sigma/sqrt(n)), t='l', lwd=2, col=1, main="", ylab="")
```

:::

## Intervalos de Confiança

Ao saber a distribuição de amostragem de um determinado estimador $T$ (como a média amostral $\bar X$), podemos construir um intervalo de confiança para o parâmetro populacional desconhecido $\theta$. 

O objetivo será obter $a<b$ tais que
$$\begin{align*}
  \Pr(a\leq \theta \leq b) = 1-\alpha,
\end{align*}$$
onde $\gamma=1-\alpha$ é o nível de confiança do intervalo.

  * $a$ é o limite inferior
  * $b$ é o limite superior
  * Essas são as quantidades aleatórias. Seus valores dependem da amostra particular observada $\mathbf x$.
  * A distância $b-a$, a amplitude do intervalo, é tão maior quanto mais próximo de 1 for o nível de confiança $\gamma=1-\alpha$. 
  * Chamamos $\alpha$ de nível de significância. O nível de confiança é $\gamma = 1-\alpha$, a probabilidade de o intervalo $[a,b]$ conter o verdadeiro parâmetro.

### Intervalo de confiança para a média populacional

Seja $y = \{Y_1,Y_2,\dots,Y_n\}$ uma amostra aleatória retirada da população $Y$ cuja verdadeira média é desconhecida.

Sabemos da discussão sobre a distribuição da média amostral que $E(\bar Y)=\mu$, e $Var(\bar Y)=\sigma^2/n$. O TLC nos garante que, conforme $n$ cresce,
\begin{align*}
  \bar Y \sim N(\mu, \sigma^2/n)
\end{align*}

Então a transformação
\begin{align*}
  Z = \frac{\bar Y - \mu}{\sigma/\sqrt{n}}
\end{align*}
tem média zero e variância 1. Ou seja,
\begin{align*}
  Z \sim N(0,1).
\end{align*}

Escreva
\begin{align*}
  \Pr(-c \leq Z \leq c) = 1-\alpha
\end{align*}

Perceba que $c=\Phi^{-1}\left(1-\alpha/2\right)$ ou `qnorm(1-alpha/2)`.

$$\begin{align*}
  \Pr\left(-\Phi^{-1}\left(1-\frac{\alpha}{2}\right) \leq \frac{\bar Y-\mu}{\sigma/\sqrt{n}}\leq\Phi^{-1}\left(1-\frac{\alpha}{2}\right)\right) = \gamma
\end{align*}$$

Isolando o parâmetro $\mu$ entre as desigualdades, obtemos o intervalo de confiança para a média populacional.

::: {.callout-tip title="Intervalo de confiança para a média populacional"}
  Se $x=\{x_1,\dots,x_n\}$ é uma amostra aleatória retirada de uma população $X$ com média $\mu$ desconhecida e variância $\sigma^2$ conhecida, então
$$  \begin{align*}
    \Pr\left(\bar x - \frac{\sigma}{\sqrt{n}} \Phi^{-1}\left(1-\frac{\alpha}{2}\right) \leq \mu \leq \bar x + \frac{\sigma}{\sqrt{n}} \Phi^{-1}\left(1-\frac{\alpha}{2}\right)\right) = \gamma
  \end{align*}$$
  
```{r}
#| eval: false

limite_inferior = mean(x) - (sigma/sqrt(n))*qnorm(1-alpha/2)
limite_superior = mean(x) + (sigma/sqrt(n))*qnorm(1-alpha/2)

```
:::

::: {.callout-note title="Exemplo intervalo de confiança" collapse=true}
Uma máquina enche pacotes de café com uma variância igual a 100 $g^2$. Ela estava regulada para encher os pacotes com 500 g, em média. Agora, ela se desregulou, e queremos saber qual a nova média $\mu$. 

Uma amostra de 25 pacotes apresentou uma média igual a 485 g. Vamos construir um intervalo de confiança, nível de confiança de $\gamma=95\%$ para $\mu$.

```{r}
gamma = 0.95
n = 25
xbar = 485
sigma=sqrt(100)
IC = c(xbar-(sigma/sqrt(n))*qnorm((1+gamma)/2),
       xbar+(sigma/sqrt(n))*qnorm((1+gamma)/2))
IC
```


:::

::: {.callout-note title="Simulações de intervalos de confiança para a média populacional" collapse=true}

```{r}
#| message: false
#| warning: false
#| tidy: false
mu = 2 #<1>
sigma = 3 #<1>
n = 20 #<2>
conf.level = 0.8 #<3>
MSim = 10 #<4>
xMedias = rep(0,MSim) #<4>
for (jsample in 1:MSim){
  sim = rnorm(n,mean=mu,sd=sigma)
  xMedias[jsample] = mean(sim)
}
IC = matrix(ncol=2,nrow=MSim)
IC[,1] = xMedias-(sigma/sqrt(n))*qnorm((1+conf.level)/2)
IC[,2] = xMedias+(sigma/sqrt(n))*qnorm((1+conf.level)/2)
require(dplyr)
non.viol = sum(dplyr::between(rep(mu,MSim),IC[,1],IC[,2]))
require(plotrix)
main = paste(non.viol,"de",length(xMedias),"ICs contém a verdadeira média")
plotrix::plotCI(1:length(xMedias),xMedias,ui=IC[,2], li=IC[,1],
       main=main)
abline(h=mu)
```
1. Verdadeiros parâmetros $\mu$ e $\sigma$. 
2. Tamanho amostral $n$ de cada simulação.
3. Quantidade $MSim$ de simulações. Inicializamos um vetor para guardar as estimativas.


```{r}
#| message: false
#| warning: false
#| echo: false
conf.level = 0.8
mu = 2
sigma = 3
n = 20
xMedias = rep(0,100)
for (jsample in 1:length(xMedias)){
  sim = rnorm(20,mean=mu,sd=sigma)
  xMedias[jsample] = mean(sim)
}
IC = matrix(ncol=2,nrow=length(xMedias))
IC[,1] = xMedias-(sigma/sqrt(n))*qnorm((1+conf.level)/2)
IC[,2] = xMedias+(sigma/sqrt(n))*qnorm((1+conf.level)/2)
require(dplyr)
non.viol = sum(between(rep(mu,length(xMedias)),IC[,1],IC[,2]))
require(plotrix)
main = paste(non.viol,"de",length(xMedias),"ICs contém a verdadeira média")
plotCI(1:length(xMedias),xMedias,ui=IC[,2], li=IC[,1],
       main=main)
abline(h=mu)
```
:::

### Intervalo de confiança para a proporção populacional

Um caso de extremo interesse é o caso da infe

::: {.callout-note title="Por que as pesquisas de intenção de voto erram tanto?" collapse=true}
TODO.
TL;DR:
Não são baseadas em amostras aleatórias como na definição acima, por vários motivos. Especialmente os intervalos de confiança ("margem de erro") são completa ficção. 
:::

### Intervalo de confiança para a média quando a verdadeira variância é desconhecida

Na maior parte das aplicações práticas, não sabemos previamente a verdadeira variância populacional $\sigma^2$. A solução é usar uma estimativa amostral para a variância, como
\begin{align*}
  S^2 = \sum^n_{j=1}\frac{(X_j-\bar X)^2}{n-1}.
\end{align*}

Nesse caso, a média amostral $\bar X$, depois de padronizada, tem distribuição que se aproxima de uma t-Student com $n-1$ graus de liberdade conforme $n$ cresce, não uma Normal Padrão.

\begin{align*}
  T = \frac{\bar X - \mu}{S/\sqrt{n}} \sim t(n-1).
\end{align*}

Nesse caso os extremos do intervalo de confiança têm a forma
\begin{align*}
  \bar X \pm \frac{S}{\sqrt{n}}F^{-1}_{n-1}\left(\frac{1+\gamma}{2}\right)
\end{align*}
onde $F^{-1}_m$ é a função quantílica de uma distribuição t-Student com $m$ graus de liberdade.

```{r}
#| eval: false
limite_inferior = mean(x) - (sd(x)/sqrt(n))*qt((1+gamma)/2,n-1)
limite_superior = mean(x) + (sd(x)/sqrt(n))*qt((1+gamma)/2,n-1)
```

A distribuição t-Student se aproxima de uma Normal Padrão conforme seus graus de liberdade aumentam.

Portanto a diferença entre os ICs só é relevante quando o tamanho amostral $n$ é pequeno. Para algo como $n>100$^[Depende da distribuição da variável original, mas é uma recomendação conservadora na maioria dos casos. Existe uma tradição do número $n=30$ como um "corte" para podermos aplicar o teste $t$ como o teste $z$. Ver [researchgate](https://www.researchgate.net/post/What_is_the_rationale_behind_the_magic_number_30_in_statistics#) para uma discussão informal sobre as origens da recomendação. ], podemos utilizar a estimativa $s^2$ no lugar da verdadeira variância $\sigma^2$ e consultar os quantis `qnorm((1+gamma)/2)` (ou `qnorm(1+alpha/2)`) sem muita preocupação. 

```{r}
#| echo: false

par(mgp=c(2,1,0),mar=c(3, 3, 2, 1)+0.1)
x = seq(from=-4,to=4,length=1000)
m.grid=c(1,2,5,50)
plot(x, dnorm(x), t='l', lwd=2, lty=2, main=TeX(r"( X\sim t(m) )"), ylab=TeX(r"( $f_X(x)$ )"))
lgnd = vector(length=length(m.grid))
lgnd[1] = "N(0,1)"
for (j in 1:(length(m.grid))){
  lgnd[j] = paste("t(",m.grid[j],")",sep='')
  lines(x, dt(x, df=m.grid[j]), lwd=2, lty=1, col=j+1)
}
lgnd = c("N(0,1)", lgnd)
lines(x, dnorm(x), lwd=2, col=1, lty=2)
lty.grid = c(2, rep(1, length(m.grid)))
col.grid = c(1,1:length(m.grid)+1)
legend('topright', legend = lgnd, lwd=2, lty=lty.grid, col=col.grid, cex=.8)
```



::: {.callout-note title="Intervalo de confiança para a média quando a verdadeira variância é desconhecida" collapse=true}



:::

### Comentários adicionais

::: {.callout-note title="Intervalo de confiança subótimo" collapse=true}
Os intervalos de confiança para a média cujos extremos são dados por
\begin{align*}
  \bar X \pm \frac{S}{\sqrt{n}} \Phi^{-1}\left(\frac{1+\gamma}{2}\right)
\end{align*}
são os menores intervalos (em termos da diferença entre limite superior e limite inferior) que contém a verdadeira média com probabilidade $\gamma$ (o nível de confiança do intervalo). 

É possível escolher um valor particular para `LimInf` ou `LimSup` e daí construir um intervalo não centrado na estimativa da média amostral, que vai conter a verdadeira média populacional com a mesma probabilidade $\gamma$ (mesmo nível de confiança). Mas esse intervalo não centrado seria "menos eficiente" no sentido que sua amplitude sempre será maior do que o intervalo centrado em $\bar x$. 

TODO: exemplo intervalo não ótimo.
:::

::: {.callout-note title="Intervalo de confiança para a variância de uma população Normal" collapse=true}

:::

## Propriedades de Estimadores

Um estimador $T$ do parâmetro $\theta$ é qualquer função das observações da amostra, ou seja, $T=g(X_1,\dots,X_n)$.

Uma estimativa $\hat T$ é o valor assumido pelo estimador em uma amostra particular.

### Valor esperado e viés

::: {.callout-tip title="Viés de um estimador"}
Definimos o viés do estimador $T$ com relação ao parâmetro $\theta$ como
\begin{align*}
  \text{Viés}(T,\theta) = E(T-\theta) = E(T)-\theta
\end{align*}

O estimador $T$ é dito não-enviesado com relação ao parâmetro $\theta$ se $\text{Viés}(T,\theta)=0$, ou seja, se
\begin{align*}
    E(T)=\theta,
\end{align*}
para qualquer possível valor $\theta\in\Theta$.
:::

A média amostral é estimador não enviesado da verdadeira média populacional. Como $E(\bar X)=\mu$, 
$$Vies(\bar X, \mu)=E(\bar X)-\mu=\mu-\mu=0.$$
Pode ser surpreendente à primeira vista, mas o estimador 
$$\hat\sigma^2 = \frac{1}{n}\sum^n_{j=1}(\bar x-x_j)^2,$$
não é um estimador não enviesado para a verdadeira variãncia $\sigma^2.

O estimador não enviesado para a variância é
$$ s^2 = \frac{1}{n-1}\sum^n_{j=1}(\bar x-x_j)^2 $$

::: {.callout-note title="Dois estimadores para a variância: $\hat\sigma^2$ e $S^2$" collapse=true}
Seja $X$ uma v.a. com média $\mu$ e variância $\sigma^2$. Tome uma amostra aleatória com tamanho $n$, $x = (x_1,\dots,x_n)$. Um possível estimador para a variância populacional $\sigma^2$, talvez o mais natural, é
\begin{align*}
  \hat \sigma^2 = \frac{1}{n}\sum^n_{j=1}(x_j - \bar x)^2.
\end{align*}

Qual o valor esperado do estimador $\hat \sigma^2$? Qual $E(\hat \sigma^2)$?

Primeiro, lembre que conseguimos escrever
\begin{align*}
  \hat \sigma^2 = \frac{1}{n} \left[\sum^n_j x_j^2 - n\bar x^2\right] = \frac{\sum^n_j x^2_j}{n}-\bar x^2.
\end{align*}   
Agora tome $E(\hat \sigma^2)$.
\begin{align*}
  E(\hat \sigma^2) = \frac{\sum^n_iE(x^2_j)}{n}-E(\bar x^2).
\end{align*}
Aqui usamos o fato da amostra ser iid para escrever
\begin{align*}
  E(\hat \sigma^2) = E(X^2)-E(\bar X^2).
\end{align*}
Lembre que $Var(Y)=E(Y^2)-E(Y)^2$.
\begin{align*}
  Var(\bar X) &= E(\bar X^2) - E(\bar X)^2, \\
  \sigma^2/n &= E(\bar X^2) - \mu^2, \\
  E(\bar X^2) &= \mu^2 + \sigma^2/n.
\end{align*}
Similarmente, 
\begin{align*}
    Var(X) &= E(X^2)-E(X)^2, \\
    \sigma^2&= E(X^2)-\mu^2, \\
    E(X^2) &= \sigma^2+\mu^2.
\end{align*}
Portanto,
\begin{align*}
  E(\hat\sigma^2) = (\sigma^2+\mu^2) - \left(\frac{\sigma^2}{n}+\mu^2\right), \\
  &= \frac{n-1}{n}\sigma^2
\end{align*}
E assim percebemos que o estimador $\hat\sigma^2$ é viesado porque $E(\hat\sigma^2)\neq \sigma^2$. Em particular, $(n-1)/n<1$ e vemos que $\hat\sigma^2$ subestima sistematicamente a variância populacional. Especificamente, 
\begin{align*}
  \text{Viés}(\hat\sigma^2) = E(\hat\sigma^2)-\sigma^2 =  -\frac{\sigma^2}{n}.
\end{align*}
O cálculo de $E(\hat\sigma^2)$ imediatamente nos sugere uma ``correção'' no estimador. Lembre que $aE(X)=E(aX)$, então
\begin{align*}
  \frac{n}{n-1}E(\hat{\sigma}^2) = \frac{n}{n-1}\frac{n-1}{n}\sigma^2=\sigma^2
\end{align*}
    Mas 
\begin{align*}
  \frac{n}{n-1}\hat \sigma^2 = \frac{1}{n-1}\sum^n_{j=1}(x_j-\bar x)^2 \equiv s^2
\end{align*}
Costumamos nos referir a essa versão não-enviesada do estimador da variância como $s^2$ ou $S^2$.
:::

Isso acontece porque nós "perdemos um grau de liberdade" ao estimar a média. Ver (@nte-grausdeliberdade) para discussão.   

::: {#nte-grausdeliberdade .callout-note title="O que é um grau de liberdade?" collapse=true}

Os parâmetros das distribuições Qui-quadrado e t-Student são chamados "graus de liberdade". Sua interpretação em termos práticos não é tão óbvia como no caso das outras distribuições de probabilidade mencionadas.

Olhando () vemos que os graus de liberdade $m$ são a quantidade de normais padrão ao quadrado $z^2$ somadas para obter a distribuição Chi-quadrado $\chi_m$.

Tome uma amostra de tamanho $n=4$. Fixe $\bar x=1$ 
  $$ \bar x = \frac{1}{4}(x_1+x_2+x_3+x_4) $$
Vamos escolher valores possíveis para $x_1,x_2,x_3,x_4$. Diga
  - $x_1 = 0$, $x_2=1$, $x_3=2$
  - Para termos $\bar x=1$, último valor da amostra, $x_4$ tem, necessáriamente que ser $x_4=1$.
  
Ao fixar a média amostral, determinamos um dos valores da amostra (depois de sorteados todos os demais). 

Quando estimamos a variância amostral usando
$$ \hat\sigma^2 = \frac{1}{n}\sum^n_{j=1}(x_j - \bar x)^2 \text{ ou }  s^2= \frac{n}{n-1}\hat\sigma^2,$$
Para calcular a variância nós precisamos subtraír a média de cada observação, a transformação $u_j = X_j-\bar X$. Isso tem o efeito de fixar a média da nova amostra $\{u_j\}^n_{j=1}$ em $\bar u=0$. E $\hat\sigma^2$ é a média do quadrado da variável $u$, $\sum^n_{j=1}u^2/n$.

Podemos interpretar os graus de liberdade como uma medida de quanto a dispersão dos dados efetivamente define o valor da variância amostral, contra quanto a estimativa da média amostral define esse valor.

No nosso contexto atual nós perdemos apenas um grau de liberdade ao estimar a média e o efeito parece inócuo. 

Mas em aplicações como regressão linear, onde usamos um conjunto de variáveis independentes para explicar uma variável depenente,

$$ y_j = \beta_1x_{t,1} + \beta_2x_{t,2} + \dots + \beta_px_{t,p} + u_j $$
Nós perdemos um grau de liberdade para cada coovariada $1,\dots,p$. Então se $p$ próximo de $n$, nossa estimativa para a variância do termo de erro $u_j$ vai ser baseada em poucas "observações efetivamente livres".  


:::


O viés é uma propriedade do estimador $T_n$ em repetidas amostras de tamanho $n$. 

::: {.callout-note title="Simulação de estimativas para a variância: $\hat\sigma^2$ e $S^2$" collapse=true}
TODO
:::

::: {.callout-note title="$s$ não é estimador não enviesado do desvio padrão $\sigma$" collapse=true}
Embora $s^2$ seja estimador não enviesado da variância $\sigma^2$, $s=\sqrt{s^2}$ não é estimador não enviesado do parâmetro de desvio-padrão $\sigma=\sqrt{\sigma^2}$. Essa é uma decorrência direta da desigualdade de Jensen.

$$ s^2 = \frac{\sum (x_j-\bar x)^2}{n-1} $$
$$ s = \sqrt{\frac{\sum (x_j-\bar x)^2}{n-1}} \\ E(s^2) = \sigma^2$$
$$ E\left(\sqrt{s^2}\right) \leq \sqrt{E(s^2)} \\
E(\sqrt{s^2})\leq \sqrt{\sigma^2}$$
:::

### Variância, eficiência relativa, e erro quadrático médio

A variância de um estimador é simplesmente $Var(T)=E(T^2)-E(T)^2$.


### Consistência

É interessante termos um estimador não enviesado (...).

Mas na maior parte das aplicações práticas, especialmente em economia, não temos acesso a repetidas amostras aleatórias retiradas da população subjacente. 

Gostaríamos de pode afirmar algo sobre a precisão de nossas estimativas dada uma única amostra $x=\{X_1,\dots,X_n\}$ conforme o tamanho amostral $n$ cresce cada vez mais. 


::: {.callout-tip title="Consistência de um estimador"}
Uma sequência $\{T_n\}$ de estimadores [calculados com base em amostras de tamanho $n$] é consistente se, conforme $n\rightarrow\infty$,

\begin{align*}
  \lim_{n\rightarrow\infty} \Pr\left(|T_n-\theta|>\varepsilon\right)=0
\end{align*}
Equivalentemente, podemos escrever 
\begin{align*}
  \lim_{n\rightarrow\infty} \Pr\left(|T_n-\theta|<\varepsilon\right)=1
\end{align*}
:::
Essa condição quer dizer que o estimador $T_n$ converge em probabilidade para o verdadeiro valor do parâmetro $\theta$ conforme $n$ cresce. Às vezes escrevemos
\begin{align*}
    \text{\plim}(T_n)=\theta,
\end{align*}
como uma notação mais curta.

Uma sequência $\{T_n\}$ de estimadores de $\theta$ é consistente se, simultaneamente,
\begin{align*}
  &\lim_{n\rightarrow\infty} E(T_n)=\theta, \text{ e }\\
  &\lim_{n\rightarrow\infty} Var(T_n)=0.
\end{align*}

::: {.callout-note title="Consistência de $\bar X$" collapse=true}
Seja $X$ uma v.a. com média $\mu$ e variância $\sigma^2$. Tome uma sequência de amostras aleatórias $x^{(n)}$ com tamanho $n$ tal que
\begin{align*}
  x^{(1)} &= \{x_1\} \\
  x^{(2)} &= \{x_1, x_2\} \\
 & \vdots\\
  x^{(n)} &= \{x_1,x_2\dots,x_n\}
\end{align*}
O estimador $\bar X$ para a média amostral é definido como
\begin{align*}
  \bar X(x^{(n)}) = \frac{\sum^n_{j=1}x_j}{n}
\end{align*}
Agora vamos ver o que acontece com $E(\bar X)$ e  $Var(\bar X)$ conforme o tamanho amostral $n$ cresce.
A condição (C1) é sempre satisfeita,  $\E(\bar X)=\mu$ para qualquer tamanho amostral $n$, então
\begin{align*}
  \lim_{n\rightarrow\infty}\E(\bar X) = \mu.
\end{align*}
A variância de $\bar X$ é dada por
\begin{align*}
  \Var(\bar X(\bm x^{(n)})) = \frac{\sigma^2}{n}
\end{align*}
Aqui também é direto ver que
\begin{align*}
  \lim_{n\rightarrow\infty} \Var(\bar X(\bm x^{(n)})) = \lim_{n\rightarrow\infty}\frac{\sigma^2}{n} = 0
\end{align*}
:::

::: {.callout-note title="Consistência de $\hat\sigma^2$" collapse=true}
Embora $\hat \sigma^2$ seja um estimador viesado, ele é consistente --- no sentido que conforme $n$ cresce, ambos viés e variância diminuem, e nossa estimativa é cada vez mais. 
Nós calculamos o valor esperado de $\hat\sigma^2$ como 
$$E(\hat\sigma^2)=\frac{n-1}{n}\sigma^2.$$
A variância de $\hat\sigma^2$ é (ver exercício [colocar-referencia])

$$Var(\hat\sigma^2)=\frac{2\sigma^4}{n}.$$
Então é direto ver que

\begin{align*}
  &\lim_{n\rightarrow\infty} E(\hat\sigma^2) = \lim_{n\rightarrow\infty} \frac{n-1}{n}\sigma^2=\sigma^2, \text{ e } \\
   &\lim_{n\rightarrow\infty} Var(\hat\sigma^2) = \lim_{n\rightarrow\infty}\frac{2\sigma^4}{n} = 0.
\end{align*}

:::



## Lei dos Grandes Números

A Lei Fraca dos Grandes Números (LFGN) garante que a estimativa da média amostral $\bar x$, calculada a partir de amostras amostras aleatórias de tamanho $n$ da população original, vai se aproximar cada vez mais da verdadeira média populacional $\mu$ conforme $n$ cresce.



See @thm-LFGN

::: {.callout-tip title="Lei Fraca dos Grandes Números"}
::: {#thm-LFGN title="Lei Fraca dos Grandes Números"}
#### Lei Fraca dos Grandes Números 

Seja $X_1,X_2,\dots$ uma sequência de variáveis aleatórias iid com $E(X_i)=\mu$ e $Var(X_i)=\sigma^2<\infty$. Defina $\bar X_n=(1/n)\sum^n_{i=1}X_i$. Então, para todo $\epsilon>0$,
\begin{align*}
  \lim_{n\rightarrow\infty} \Pr(|\bar X_n - \mu| < \epsilon) = 1.
\end{align*}
:::
:::

Uma variável aleatória $C$ com distribuição Chi-quadrado $\chi^2(v)$ e $v$ "graus de liberdade" é definida como a soma de $v$ Normais Padrão independentes. (Obs.: É relacionada com a distribuição da variância amostral.)
$$ 
  C = z^2 + z^2 + \cdots + z^2 \sim \chi^2(v) 
$$
É direto ver que $E(C)=v$ e $Var(C)=2v$ (use $E(Z^4)=3$).

```{r}
#| echo: false

par(mgp=c(2,1,0),mar=c(3, 3, 2, 1)+0.1)
x = seq(from=0.01, to=15, length=1000)
m.grid = c(1,2,3,5,10)
plot(x, dchisq(x,df=m.grid[1]), t='l', lwd=2, lty=1, col=1, ylim=c(0,0.7), main=TeX(r"( $X\sim\chi^2(m)$ )"), ylab=TeX(r"( $f_X(x)$ )"))
for (j in 2:length(m.grid)){
  lines(x, dchisq(x, df=m.grid[j]), t='l', lwd=2, lty=1, col=j)
}
lgnd = c(TeX(r"($m=1$)"),
         TeX(r"($m=2$)"),
         TeX(r"($m=3$)"),
         TeX(r"($m=5$)"),
         TeX(r"($m=10$)"))
legend('topright', legend = lgnd, lwd=2, lty=1, col=1:length(m.grid), cex=.8)
```


Vamos simular amostras aleatórias de uma v.a. Qui-quadrado com diferentes tamanhos e observar o comportamento das estimativas da média e variância.

```{r}
nmax = 10000
nmin = 50
ngrid = seq(from=nmin,to=nmax,by=50)
v = 3
Sim = rchisq(nmax,df=3)
cMeans = rep(0,length(ngrid))
j = 1
for (nobs in ngrid){
  cMeans[j] = mean(Sim[1:nobs])
  j = j+1
}
plot(ngrid, cMeans, t='l')
abline(h=v)
```

A variância de uma variável aleatória é definida como
$$
  Var(X)=E[(X-E(X))^2].
$$
Repare que se definirmos $Y=(X-E(X))^2$ então $Var(X)=E(Y)$. Portanto a LGN também se aplica às estimativas da variância.
```{r}
j = 1
cVars = rep(0,length(ngrid))
for (nobs in ngrid){
  cVars[j] = var(Sim[1:nobs])
  j = j+1
}
plot(ngrid, cVars, t='l')
abline(h=2*v)
```

De modo geral, a LGN vai garantir a consistência dos estimadores dos momentos $E(X^k)$ contanto que o momento em si seja bem definido. Isto é, $E(X^k)<\infty$.

::: {.callout-note title="Exemplo de violação da LGN" collapse=true}
Uma variável aleatória $T$ com distribuição t-Student e $m$ graus de liberdade tem função densidade de probabilidade 
\begin{align}
        f_T(t;m) = \frac{\Gamma\left(\frac{m+1}{2}\right)}{\sqrt{m\pi}\Gamma\left(\frac{m}{2}\right)}\left(1+\frac{t^2}{m}\right)^{-\left(\frac{m+1}{2}\right)}, \quad -\infty<t<\infty.
    \end{align}
A t-Student é muito relacionada com a distribuição da média amostral quando a verdadeira variância populacional é desconhecida. Conforme os graus de liberdade $m$ crescem, a distribuição t-Student se aproxima cada vez mais de uma Normal Padrão.    

Usualmente, $E(T)=0$ e $Var(T)=m/(m-2)$. Mas o k-ésimo momento $E(T^k)$ só é bem definido se $k<m$.

Neste exemplo estamos interessados no caso com $m=1$ grau de liberdade.
\begin{align}
        f_T(t;1) = \frac{\Gamma\left(1\right)}{\sqrt{\pi}\Gamma\left(\frac{1}{2}\right)}\left(1+t^2\right)^{-1}, \quad -\infty<t<\infty.
    \end{align}
Sabendo que $\Gamma(1/2)=\sqrt{\pi}$ e $\Gamma(1)=1$, 
\begin{align}
        f_T(t;1) = \frac{1}{\pi(1+t^2)}, \quad -\infty<t<\infty.
    \end{align}
Como $\text{E}(T)$ se $T\sim t(1)$ não é bem definido, o estimador da média amostral não converge para nenhum valor específico.

```{r}
set.seed(1729)
NMax = 1000
Smpl1 = rt(NMax, df=1)
Smpl2 = rt(NMax, df=1)
XBar1 = cumsum(Smpl1)/1:NMax
XBar2 = cumsum(Smpl2)/1:NMax
```
```{r}
#| echo: false
par(mgp=c(2,1,0),mar=c(3, 3, 2, 1)+0.1)
plot(1:NMax, XBar1, t='l', lwd=2, col=2, main=TeX(r"($\bar{T}(n)$, $T\sim t(1)$)"), ylab='', xlab="n")
abline(h=0, lwd=2, lty=2, col=1)
lines(1:NMax, XBar2, t='l', lwd=2, col=3)
legend('topright', col=2:3, lty=1,lw=2, legend=c('t(1)', 't(1)'),cex=.8)
```

Mesmo se aumentarmos o tamanho da amostra para 100000, não nos aproximamos de nenhum valor particular. Note também que embora as estimativas se mantenham relativamente estáveis por grandes "períodos", eventualmente uma realização tão extrema será sorteada e impactará desproporcionalmente a estimativa.

```{r}
#| echo: false
set.seed(1729)
NMax = 100000
Smpl1 = rt(NMax, df=1)
Smpl2 = rt(NMax, df=1)
XBar1 = cumsum(Smpl1)/1:NMax
XBar2 = cumsum(Smpl2)/1:NMax
par(mgp=c(2,1,0),mar=c(3, 3, 2, 1)+0.1)
plot(1:NMax, XBar1, t='l', lwd=2, col=2, main=TeX(r"($\bar{T}(n)$, $T\sim t(1)$)"), ylab='', xlab="n")
abline(h=0, lwd=2, lty=2, col=1)
lines(1:NMax, XBar2, t='l', lwd=2, col=3)
legend('topright', col=2:3, lty=1,lw=2, legend=c('t(1)', 't(1)'),cex=.8)
```

:::

## Métodos para obtenção de Estimadores

### Método dos momentos

::: {.callout-note title="Dois estimadores de momentos para o mesmo parâmetro" collapse=true}

:::

### Máxima Verossimilhança

::: {.callout-tip title="Método da Máxima Verossimilhança"}

  1. Especificamos uma distribuição  probabilidade para a variável aleatória original $X$.
  
  $$ X \sim f(x;\theta) $$
  
  2. Tomamos uma amostra aleatória de tamanho $n$ 
  
\begin{align*} 
  \mathbf x = \{x_1,x_2,\dots,x_n\}
\end{align*}

  3. Escrevemos a probabilidade de obter a amostra específica $\mathbf x$. É a função de verossimilhança do parâmetro $\theta$, tomando como dada a amostra observada $\mathbf x$.
  
  $$ \mathcal L (\theta;\mathbf x) = \prod^n_{j=1} f(x_j;\theta)$$
Repare que agora escrevemos $\mathcal L$ como uma função de $\theta$.

  4. O estimador de máxima verossimilhança $\tilde\theta$ é o valor de $\theta$ que
maximiza a função de verossimilhança $\mathcal L(\theta)$.

$$ \tilde\theta = \arg\max_{\theta\in\Theta} \mathcal L(\theta;\mathbf x) $$
A interpretação é que $\tilde\theta$ maximiza a probabilidade de sortearmos especificamente a amostra observada em $n$ sorteios da variável aleatória com distribuição $f(x;\theta)$.

:::

::: {.callout-note title="Otimização de função de uma variável real" collapse=true}
Seja $g:I\rightarrow \mathbb R$ uma função duas vezes continuamente diferenciável.

Se $g^\prime(x_0)=0$, então $x_0$ é dito ponto crítico de $g$.

O ponto crítico $x_0\in I$ é ponto de

  - máximo se $f^{\prime\prime}(x_0)<0$,
  - mínimo se $f^{\prime\prime}(x_0)>0$, e
  - inflexão se $f^{\prime\prime}(x_0)=0$.

:::


::: {.callout-note title="Estimador de MV na distribuição Exponencial" collapse=true}
Suponha $X\sim \text{Exp.}(\beta)$. Nesse caso,
\begin{align*}
  f(x;\beta)=\frac{1}{\beta}e^{-\frac{x}{\beta}}, \quad x>0, \beta>0.
\end{align*}
A função de verossimilhança fica
\begin{align*}
\mathcal L(\beta;\mathbf x) &= \prod^n_{j=1} f(x;\beta) =\prod^n_{j=1}\frac{1}{\beta}e^{-x/\beta} \\
&= \frac{1}{\beta^n}\prod^n_{j=1}e^{-x_j/\beta} = \frac{1}{\beta^n} e^{-\frac{1}{\beta}\sum^n_{j=1}x_j}
\end{align*}

Alguma álgebra nos leva a 
$$
  \frac{d\mathcal L(\lambda;\mathbf x)}{d\lambda} = \lambda^{n-1}e^{-\lambda\sum x_j}(n-\lambda\sum x_j)
$$
Então $d\mathcal L(\lambda;\mathbf x)/d\lambda=0$ quando


$$\begin{align*}
\hat\lambda = \left(\frac{\sum^n_{j=1}}{n}\right)
\end{align*}$$


```{r}
n = 10
tru.lambda = 5
smpl = rexp(n = n,rate = tru.lambda)
lik.exp = function(X,lambda){
  n = length(X)
  lik = (lambda^n)*exp(-lambda*sum(X)-n)
  return(lik)
} 
lambda.vals = seq(1,15,by=0.05)
lik.fun = sapply(lambda.vals, function(j) lik.exp(smpl,j))
par(mfrow=c(2,1))
plot(lambda.vals, lik.fun, t="l")
abline(v=1/mean(smpl))
plot(lambda.vals, log(lik.fun), t="l")
abline(v=1/mean(smpl))
```

::: 

::: {.callout-tip title="Função de log-verossimilhança"}

:::


::: {.callout-note title="Diferença entre verossimilhança e probabilidade" collapse=true}
TODO
:::

::: {.callout-note title="MLE distribuição Poisson" collapse=true}

$$ f(x;\lambda)=e^{-\lambda}\frac{\lambda^x}{x!} $$
$$\mathcal L = \prod^n_{j=1}f(x;\lambda)\\
=\prod e^{-\lambda}\frac{\lambda^{x_j}}{x_j!}\\
=e^{-n\lambda}\prod\frac{\lambda^{x_j}}{x_j!}$$
$$ \frac{d\mathcal L}{d\lambda} = Ae^{-\lambda n}\lambda^{\sum x_j - 1}\left(\sum x_j - n\lambda\right)$$
:::


## Exercícios de fixação

::: {#exr-transformacao-monotonica-extremos collapse=true}
(Transformações monotônica preservam a localização de extremos) Sejam $f:I_1\rightarrow \mathbb R$ e $g:I_2 \rightarrow\mathbb R$, com $f[I_1]\subset I_2$, funções duas vezes continuamente diferenciáveis. Mostre que, se $x_0$ é ponto de máximo ou mínimo local de $f(x)$, então também é máximo ou mínimo local de $g(f(x))$.

::: {.callout-caution collapse=true title="Solução"}

:::

:::
